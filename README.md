## 项目背景
背景1：

近年来，图像描述生成(Image Caption)领域不断发展，其主要目标是根据输入的图像信息，生成相应的文字描述，从而完成对图像内容的准确描述。为了实现图像描述的任务，常见的方法是采用编码器-解码器（encoder-decoder）的结构。这种结构可以将输入的图像信息通过编码器进行抽象和提取，得到一个表示图像特征的向量。然后，解码器将这个向量作为输入，逐步生成与图像内容相对应的文字描述。这种结构的实现中，常常使用transformer作为主体机构。

目前所发表的论文中大都是通过构建或已有的大规模的图片-空间描述数据集并结合预训练的视觉语言模型来完成这一目标，但不知是数据集中的描述文本普遍过短还是视觉语言模型的能力不足，导致在我所实验数据集上的效果不佳，也可以说是几乎没有效果。我思考的原因可能是多数论文中的模型更倾向于针对某一个数据集，针对该数据集进行了精心的调优，使其在面对其他更多领域的数据集时难以给出满意的表现（当然，复现论文也比较消耗精力，也有可能是我所看的这些方法的不够前沿，但无论如何，我想使用一种更为简单有效的方法来完成这一事情）。

背景2：

在跨视角的图像匹配这一任务中，也就是在同一数据集中包含同一目标的不同视角图像，例如：
![1](https://github.com/user-attachments/assets/60b928b5-59ad-488c-b147-b943a2a03d01)
![0847](https://github.com/user-attachments/assets/6c45340a-9950-4e69-b4b2-deeb5e40b45f)
![0847](https://github.com/user-attachments/assets/a69e0877-215f-4a8f-8c30-6c5514c29be6)


## 项目动机




问题1：  

该项目代码有所改变，我将尽快修改代码，并给出readme文件
